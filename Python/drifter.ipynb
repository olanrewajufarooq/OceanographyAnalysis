{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "# Python-based Libraries\n",
    "from datetime import timedelta, datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Data Analysis Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graph plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplcols\n",
    "from matplotlib import cm\n",
    "import folium\n",
    "import branca.colormap as bcm\n",
    "\n",
    "    \n",
    "# Filtering Data before Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drifters:\n",
    "    \n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "        self.data = {}\n",
    "        self.data_info = {}\n",
    "        \n",
    "    def read_data(self, path):\n",
    "        # Define Possible encodings\n",
    "        encodings = [\"UTF-8\", \"UTF-16 LE\"]\n",
    "\n",
    "        # Check if path is a filepath or directory path\n",
    "        if Path(path).is_file(): \n",
    "            # Get file name from path\n",
    "            filename = path.split('/')[-1].split('.')[-2] \n",
    "            # Define a variable to track file decoding\n",
    "            encoding_gotten = False\n",
    "            for encoding_value in encodings:\n",
    "                try:\n",
    "                    data_value = pd.read_csv(path, encoding=encoding_value)\n",
    "                    # Catch special errors whereby the encoding is wrong but the data is read.\n",
    "                    if \"Unnamed: 1\" in data_value.columns: \n",
    "                        continue\n",
    "                    encoding_gotten = True\n",
    "                    break # Stop trying if the options had worked\n",
    "                except:\n",
    "                    pass # Continue trying if the option didn't work\n",
    "\n",
    "            if encoding_gotten == True:\n",
    "                data_to_keep = [['DeviceDateTime','Latitude','Longitude'], \n",
    "                            ['Position time (UTC)', 'Latitude (째)', 'Longitude (째)'],\n",
    "                            ]\n",
    "                for dtk_val in data_to_keep:\n",
    "                    try:\n",
    "                        # Select Data to Keep\n",
    "                        data_value = data_value[dtk_val]\n",
    "                        # Drop columns with missing values\n",
    "                        data_value.dropna(axis=0)\n",
    "                        # Rename Header Columns\n",
    "                        data_value.columns = ['DateTime','Latitude','Longitude']\n",
    "                        # Convert DateTime\n",
    "                        data_value['DateTime'] = pd.to_datetime(data_value['DateTime'])\n",
    "                        # Store DF in the object instance 'data'\n",
    "                        self.data[filename] = data_value\n",
    "                        \n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                print(f\"Encoding error. The encoding for {filename} data should be checked.\")\n",
    "\n",
    "        elif Path(path).is_dir():    \n",
    "        # 1. Read the files in the given path\n",
    "            file_names = os.listdir(path)\n",
    "\n",
    "            # 2. Read each file into pandas dataframe\n",
    "            for file in file_names:\n",
    "                filepath = path+'/'+file\n",
    "                filename = file.split('.')[-2] \n",
    "                # Define a variable to track file decoding\n",
    "                encoding_gotten = False\n",
    "                for encoding_value in encodings:\n",
    "                    try:\n",
    "                        data_value = pd.read_csv(filepath, encoding=encoding_value)\n",
    "                        # Catch special errors whereby the encoding is wrong but the data is read.\n",
    "                        if \"Unnamed: 1\" in data_value.columns: \n",
    "                            continue\n",
    "                        encoding_gotten = True\n",
    "                        break # Stop trying if the options had worked\n",
    "                    except:\n",
    "                        pass # Continue trying if the option didn't work\n",
    "\n",
    "                # Store the data in the \"data\" dictionary\n",
    "                if encoding_gotten == True:\n",
    "                    data_to_keep = [['DeviceDateTime','Latitude','Longitude'], \n",
    "                               ['Position time (UTC)', 'Latitude (째)', 'Longitude (째)'],\n",
    "                                ]\n",
    "                    for dtk_val in data_to_keep:\n",
    "                        try:\n",
    "                            # Select Data to Keep\n",
    "                            data_value = data_value[dtk_val]\n",
    "                            # Drop columns with missing values\n",
    "                            data_value.dropna(axis=0)\n",
    "                            # Rename Header Columns\n",
    "                            data_value.columns = ['DateTime','Latitude','Longitude']\n",
    "                            # Convert DateTime\n",
    "                            data_value['DateTime'] = pd.to_datetime(data_value['DateTime'])\n",
    "                            # Store DF in the object instance 'data'\n",
    "                            self.data[filename] = data_value\n",
    "                        \n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                else:\n",
    "                    print(f\"Encoding error. The encoding for {filepath} data should be checked.\")\n",
    "    \n",
    "    def read_logsheet(self, file_path):\n",
    "        \n",
    "        self.logsheet = pd.read_csv(file_path)\n",
    "        \n",
    "        self.logsheet.dropna(axis=0, inplace=True)\n",
    "        \n",
    "        self.logsheet[\"DepDateTime\"] = self.logsheet.apply(lambda x: str(x[\"DepDate\"]) + \" \" + str(x[\"DepTime\"]), axis = 1)\n",
    "        self.logsheet[\"RecovDateTime\"] = self.logsheet.apply(lambda x: str(x[\"RecovDate\"]) + \" \" + str(x[\"RecovTime\"]), axis = 1)\n",
    "        \n",
    "        self.logsheet.drop(columns=[\"DepDate\", \"DepTime\", \"RecovDate\", \"RecovTime\"], inplace=True)\n",
    "        \n",
    "        self.logsheet[\"DepDateTime\"] = pd.to_datetime(self.logsheet[\"DepDateTime\"])\n",
    "        self.logsheet[\"RecovDateTime\"] = pd.to_datetime(self.logsheet[\"RecovDateTime\"])\n",
    "        \n",
    "        for key in self.data.keys():\n",
    "            station = self.logsheet[\"Station\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            drog_depth = self.logsheet[\"DrogDepth\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            \n",
    "            dep_datetime = self.logsheet[\"DepDateTime\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            recov_datetime = self.logsheet[\"RecovDateTime\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            dep_long = self.logsheet[\"DepLong\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            dep_lat = self.logsheet[\"DepLat\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            recov_long = self.logsheet[\"RecovLong\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            recov_lat = self.logsheet[\"RecovLat\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            \n",
    "            info = {\"Station\":station, \"DrogDepth\":drog_depth, \"DepDateTime\":dep_datetime, \"RecovDateTime\":recov_datetime,\n",
    "                    \"DepLong\":dep_long, \"DepLat\":dep_lat, \"RecovLong\":recov_long, \"RecovLat\":recov_lat}\n",
    "            \n",
    "            self.data_info[key] = info\n",
    "    \n",
    "    def time_shift(self, shift_amount:float = 1):\n",
    "        for key in self.data.keys():\n",
    "            self.data[key][\"DateTime\"] = self.data[key][\"DateTime\"] + timedelta(hours = shift_amount)\n",
    "    \n",
    "    def extract_data(self):\n",
    "        for key in self.data.keys():\n",
    "            # Get the start and end time for each experiment\n",
    "            deploy_time = self.data_info[key]['DepDateTime']\n",
    "            recov_time = self.data_info[key]['RecovDateTime']\n",
    "\n",
    "            # Extract experiment data within the start and end time \n",
    "            # bool = np.array([(deploy_time < self.data[key][\"DateTime\"][i] < recov_time) for i in range(len(self.data[key]))]).flatten()\n",
    "            bool = np.array([(deploy_time < pos_time < recov_time) for pos_time in self.data[key][\"DateTime\"]]).flatten()\n",
    "            \n",
    "            self.data[key] = self.data[key][bool]\n",
    "            self.data[key].reset_index(inplace = True)\n",
    "        \n",
    "    \n",
    "    def compute_velocity(self):\n",
    "        R = 6373.0\n",
    "        for key in self.data.keys():\n",
    "            # data = self.data[key]\n",
    "            # select columns for latitude, longtitude and time\n",
    "            df_lat = np.array(self.data[key][\"Latitude\"])\n",
    "            df_long = np.array(self.data[key][\"Longitude\"])\n",
    "            df_time = self.data[key][\"DateTime\"]\n",
    "            \n",
    "            # compute delta time\n",
    "            d_time = np.array(df_time[1:]) - df_time[:-1]\n",
    "            d_time = d_time.apply(lambda dt_i : dt_i.seconds/60.0)\n",
    "            \n",
    "            # compute distance from lat,long\n",
    "            rad_lat = np.radians(df_lat)\n",
    "            rad_long = np.radians(df_long)\n",
    "            d_lat = rad_lat[1:] - np.array(rad_lat[:-1])\n",
    "            d_long = rad_long[1:] - np.array(rad_long[:-1]) \n",
    "            \n",
    "            a = np.sin(d_lat / 2.0)**2 + np.cos(rad_lat[:-1]) * np.cos(rad_lat[1:]) * np.sin(d_long / 2)**2\n",
    "            c = 2.0 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "            distance = R * c\n",
    "            velocity = ( distance*1000.0 )/( 60.0*np.array(d_time, dtype=float) )\n",
    "            # Remove the first row\n",
    "            self.data[key]= self.data[key].iloc[1:]\n",
    "            self.data[key][\"Velocity\"] = pd.Series(velocity)\n",
    "    \n",
    "    def plot_trajectories(self, saving_path=None):\n",
    "        \n",
    "        def velocity_extremes(data):\n",
    "            min_vel = np.inf\n",
    "            max_vel = -np.inf\n",
    "\n",
    "            for key in data.keys():\n",
    "                min_vel_val = np.min(data[key][\"Velocity\"])\n",
    "                max_vel_val = np.max(data[key][\"Velocity\"])\n",
    "\n",
    "                min_vel = min_vel_val if min_vel_val < min_vel else min_vel\n",
    "                max_vel = max_vel_val if max_vel_val > max_vel else max_vel\n",
    "\n",
    "            return min_vel, max_vel\n",
    "        \n",
    "        def graph_centre(data):\n",
    "            min_lat = 100\n",
    "            max_lat = 0\n",
    "            min_lon = 100\n",
    "            max_lon = 0\n",
    "\n",
    "            for key in data.keys():\n",
    "                min_lat_val = np.min(data[key][\"Latitude\"])\n",
    "                min_lon_val = np.min(data[key][\"Longitude\"])\n",
    "                max_lat_val = np.max(data[key][\"Latitude\"])\n",
    "                max_lon_val = np.max(data[key][\"Longitude\"])\n",
    "\n",
    "                min_lat = min_lat_val if min_lat_val < min_lat else min_lat\n",
    "                max_lat = max_lat_val if max_lat_val > max_lat else max_lat\n",
    "                min_lon = min_lon_val if min_lon_val < min_lon else min_lon\n",
    "                max_lon = max_lon_val if max_lon_val > max_lon else max_lon\n",
    "\n",
    "            lat_centre = np.average([min_lat, max_lat])\n",
    "            lon_centre= np.average([min_lon, max_lon])\n",
    "\n",
    "            return lat_centre, lon_centre\n",
    "        \n",
    "        def plot_vel_traj(df_map, df, name, drog_depth, color_mapper):\n",
    "            \n",
    "            colors = ['#ffff00', '#ffffff', '#0000ff']\n",
    "            drog_depths = np.unique(self.logsheet[\"DrogDepth\"])\n",
    "            \n",
    "            drog_depth_found = False\n",
    "            count = 0\n",
    "            for depth in drog_depths:\n",
    "                if drog_depth == depth:\n",
    "                    drifter_col, drifter_info = [colors[count], f'{drog_depth}m Depth']\n",
    "                    drog_depth_found = False\n",
    "                    break\n",
    "                count += 1\n",
    "                \n",
    "            if not drog_depth_found:\n",
    "                drifter_col, drifter_info = ['#000000', 'Unknown']\n",
    "                \n",
    "            feature_group = folium.FeatureGroup(f\"{name}\\n{drifter_info}\")\n",
    "            \n",
    "            folium.Marker([ df[\"Latitude\"].iloc[-1]+0.001, df[\"Longitude\"].iloc[-1]-0.001 ],\n",
    "                    #popup = folium.Popup(f'{key}', parse_html=True),\n",
    "                    icon=folium.DivIcon(html=f\"\"\"<div style=\"font-family: courier new; color: blue\">{f\"{name}\"}</div>\"\"\")\n",
    "                    ).add_to(feature_group )\n",
    "            \n",
    "            start = False\n",
    "\n",
    "            for lat, lon in zip(df[\"Latitude\"], df[\"Longitude\"]):\n",
    "                if start: line = folium.PolyLine([[prev_lat, prev_long], [lat, lon]], color='white', weight=2).add_to(feature_group )\n",
    "                else: start = True\n",
    "\n",
    "                prev_lat = lat\n",
    "                prev_long = lon\n",
    "\n",
    "\n",
    "            for lat, lon, vel in zip(df[\"Latitude\"], df[\"Longitude\"], df[\"Velocity\"]):\n",
    "                circlemarker = folium.CircleMarker(location=(lat,lon), radius=6,  \n",
    "                                    fill=True, fill_color=color_mapper(vel)[:7], fill_opacity=0.9, \n",
    "                                    stroke=True, color=drifter_col, weight=1.5, opacity=0.9,\n",
    "                                    popup=folium.Popup(f\"{round(vel, 4)} m/s\"),\n",
    "                                ).add_to(feature_group )\n",
    "                print(color_mapper(vel)[:7])\n",
    "                \n",
    "            feature_group.add_to(df_map)\n",
    "\n",
    "            return df_map\n",
    "        \n",
    "        def visualize_drifters(data, data_info):\n",
    "            \n",
    "            # Initializing the Map\n",
    "            plot_map = folium.Map(location = graph_centre(data), zoom_start = 14.5, position=\"absolute\", width='100%', height=\"100%\", \n",
    "                                left='0%', top='0%', border=None, min_zoom=13, max_zoom=17)\n",
    "\n",
    "            # position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen\n",
    "\n",
    "            # Plotting the location and velocities into the Map\n",
    "            min_vel, max_vel = velocity_extremes(data)\n",
    "\n",
    "            color_gradients = ['orange', 'red']\n",
    "            velcolmap = bcm.LinearColormap(color_gradients, vmin=min_vel, vmax=max_vel, caption = 'Velocity (m/s)') # The velocity scale\n",
    "            velcolmap.add_to(plot_map)\n",
    "\n",
    "            for key in data.keys():\n",
    "                drog_depth = data_info[key][\"DrogDepth\"]\n",
    "                plot_map = plot_vel_traj(plot_map, data[key], key, drog_depth, velcolmap)\n",
    "\n",
    "            folium.LayerControl().add_to(plot_map)\n",
    "            return plot_map\n",
    "        \n",
    "        self.map = visualize_drifters(self.data, self.data_info)\n",
    "        \n",
    "        if saving_path:\n",
    "            self.map.save(f\"{saving_path}/Graphs/TrajAndVelocity.html\")\n",
    "            \n",
    "    def plot_boxplot(self):\n",
    "        pass\n",
    "    \n",
    "    def plot_lineplot(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code\n",
    "working_dir = \"Data/2022/Day2\"\n",
    "D = Drifters(\"2022\")\n",
    "D.read_data(f\"{working_dir}/drifters/\")\n",
    "D.read_logsheet(f\"{working_dir}/drifters-logsheet.csv\")\n",
    "# D.data[\"274\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D.logsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.time_shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D.data[\"274\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.extract_data()\n",
    "# D.data[\"274\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.time_shift(shift_amount=2)\n",
    "# D.data[\"274\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olanr\\AppData\\Local\\Temp\\ipykernel_39500\\4274634666.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[key][\"Velocity\"] = pd.Series(velocity)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Velocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-10-12 12:10:00</td>\n",
       "      <td>43.07711</td>\n",
       "      <td>5.97130</td>\n",
       "      <td>0.122558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-10-12 12:20:00</td>\n",
       "      <td>43.07718</td>\n",
       "      <td>5.97055</td>\n",
       "      <td>0.124899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2022-10-12 12:30:00</td>\n",
       "      <td>43.07725</td>\n",
       "      <td>5.96965</td>\n",
       "      <td>0.111592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2022-10-12 12:40:00</td>\n",
       "      <td>43.07736</td>\n",
       "      <td>5.96874</td>\n",
       "      <td>0.113137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-10-12 12:50:00</td>\n",
       "      <td>43.07742</td>\n",
       "      <td>5.96792</td>\n",
       "      <td>0.097512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>2022-10-12 13:00:00</td>\n",
       "      <td>43.07749</td>\n",
       "      <td>5.96709</td>\n",
       "      <td>0.085388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>2022-10-12 13:10:00</td>\n",
       "      <td>43.07750</td>\n",
       "      <td>5.96637</td>\n",
       "      <td>0.092078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>2022-10-12 13:20:00</td>\n",
       "      <td>43.07748</td>\n",
       "      <td>5.96574</td>\n",
       "      <td>0.101383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>2022-10-12 13:30:00</td>\n",
       "      <td>43.07748</td>\n",
       "      <td>5.96506</td>\n",
       "      <td>0.110077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>2022-10-12 13:40:00</td>\n",
       "      <td>43.07763</td>\n",
       "      <td>5.96434</td>\n",
       "      <td>0.122110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>2022-10-12 13:50:00</td>\n",
       "      <td>43.07777</td>\n",
       "      <td>5.96355</td>\n",
       "      <td>0.110401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>2022-10-12 14:00:00</td>\n",
       "      <td>43.07799</td>\n",
       "      <td>5.96270</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>2022-10-12 14:10:00</td>\n",
       "      <td>43.07824</td>\n",
       "      <td>5.96196</td>\n",
       "      <td>0.124936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>2022-10-12 14:20:00</td>\n",
       "      <td>43.07864</td>\n",
       "      <td>5.96110</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>2022-10-12 14:30:00</td>\n",
       "      <td>43.07900</td>\n",
       "      <td>5.96032</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index            DateTime  Latitude  Longitude  Velocity\n",
       "2       5 2022-10-12 12:10:00  43.07711    5.97130  0.122558\n",
       "3       6 2022-10-12 12:20:00  43.07718    5.97055  0.124899\n",
       "4       7 2022-10-12 12:30:00  43.07725    5.96965  0.111592\n",
       "5       8 2022-10-12 12:40:00  43.07736    5.96874  0.113137\n",
       "6       9 2022-10-12 12:50:00  43.07742    5.96792  0.097512\n",
       "7      10 2022-10-12 13:00:00  43.07749    5.96709  0.085388\n",
       "8      11 2022-10-12 13:10:00  43.07750    5.96637  0.092078\n",
       "9      12 2022-10-12 13:20:00  43.07748    5.96574  0.101383\n",
       "10     13 2022-10-12 13:30:00  43.07748    5.96506  0.110077\n",
       "11     14 2022-10-12 13:40:00  43.07763    5.96434  0.122110\n",
       "12     15 2022-10-12 13:50:00  43.07777    5.96355  0.110401\n",
       "13     16 2022-10-12 14:00:00  43.07799    5.96270  0.138056\n",
       "14     17 2022-10-12 14:10:00  43.07824    5.96196  0.124936\n",
       "15     18 2022-10-12 14:20:00  43.07864    5.96110       NaN\n",
       "16     19 2022-10-12 14:30:00  43.07900    5.96032       NaN"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.compute_velocity()\n",
    "D.data[\"274\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ff1d00ff\n",
      "yes\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa500\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa400\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa500\n",
      "#ffa400\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Thresholds are not sorted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m D\u001b[39m.\u001b[39;49mplot_trajectories()\n\u001b[0;32m      2\u001b[0m D\u001b[39m.\u001b[39mmap\n",
      "Cell \u001b[1;32mIn [39], line 284\u001b[0m, in \u001b[0;36mDrifters.plot_trajectories\u001b[1;34m(self, saving_path)\u001b[0m\n\u001b[0;32m    281\u001b[0m     folium\u001b[39m.\u001b[39mLayerControl()\u001b[39m.\u001b[39madd_to(plot_map)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m plot_map\n\u001b[1;32m--> 284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap \u001b[39m=\u001b[39m visualize_drifters(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_info)\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m saving_path:\n\u001b[0;32m    287\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msaving_path\u001b[39m}\u001b[39;00m\u001b[39m/Graphs/TrajAndVelocity.html\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [39], line 279\u001b[0m, in \u001b[0;36mDrifters.plot_trajectories.<locals>.visualize_drifters\u001b[1;34m(data, data_info)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    278\u001b[0m     drog_depth \u001b[39m=\u001b[39m data_info[key][\u001b[39m\"\u001b[39m\u001b[39mDrogDepth\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 279\u001b[0m     plot_map \u001b[39m=\u001b[39m plot_vel_traj(plot_map, data[key], key, drog_depth, velcolmap)\n\u001b[0;32m    281\u001b[0m folium\u001b[39m.\u001b[39mLayerControl()\u001b[39m.\u001b[39madd_to(plot_map)\n\u001b[0;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m plot_map\n",
      "Cell \u001b[1;32mIn [39], line 252\u001b[0m, in \u001b[0;36mDrifters.plot_trajectories.<locals>.plot_vel_traj\u001b[1;34m(df_map, df, name, drog_depth, color_mapper)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39myes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m lat, lon, vel \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(df[\u001b[39m\"\u001b[39m\u001b[39mLatitude\u001b[39m\u001b[39m\"\u001b[39m], df[\u001b[39m\"\u001b[39m\u001b[39mLongitude\u001b[39m\u001b[39m\"\u001b[39m], df[\u001b[39m\"\u001b[39m\u001b[39mVelocity\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m    251\u001b[0m     circlemarker \u001b[39m=\u001b[39m folium\u001b[39m.\u001b[39mCircleMarker(location\u001b[39m=\u001b[39m(lat,lon), radius\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m,  \n\u001b[1;32m--> 252\u001b[0m                         fill\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fill_color\u001b[39m=\u001b[39mcolor_mapper(vel)[:\u001b[39m7\u001b[39m], fill_opacity\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, \n\u001b[0;32m    253\u001b[0m                         stroke\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, color\u001b[39m=\u001b[39mdrifter_col, weight\u001b[39m=\u001b[39m\u001b[39m1.5\u001b[39m, opacity\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m,\n\u001b[0;32m    254\u001b[0m                         popup\u001b[39m=\u001b[39mfolium\u001b[39m.\u001b[39mPopup(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(vel, \u001b[39m4\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m m/s\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    255\u001b[0m                     )\u001b[39m.\u001b[39madd_to(feature_group )\n\u001b[0;32m    256\u001b[0m     \u001b[39mprint\u001b[39m(color_mapper(vel)[:\u001b[39m7\u001b[39m])\n\u001b[0;32m    258\u001b[0m feature_group\u001b[39m.\u001b[39madd_to(df_map)\n",
      "File \u001b[1;32mc:\\Users\\olanr\\miniconda3\\envs\\data-analysis\\lib\\site-packages\\branca\\colormap.py:146\u001b[0m, in \u001b[0;36mColorMap.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    143\u001b[0m     \u001b[39m\"\"\"Provides the color corresponding to value `x` in the\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39m    form of a string of hexadecimal values \"#RRGGBBAA\".\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrgba_hex_str(x)\n",
      "File \u001b[1;32mc:\\Users\\olanr\\miniconda3\\envs\\data-analysis\\lib\\site-packages\\branca\\colormap.py:140\u001b[0m, in \u001b[0;36mColorMap.rgba_hex_str\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrgba_hex_str\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    137\u001b[0m     \u001b[39m\"\"\"Provides the color corresponding to value `x` in the\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    form of a string of hexadecimal values \"#RRGGBBAA\".\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m#\u001b[39m\u001b[39m%02x\u001b[39;00m\u001b[39m%02x\u001b[39;00m\u001b[39m%02x\u001b[39;00m\u001b[39m%02x\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrgba_bytes_tuple(x)\n",
      "File \u001b[1;32mc:\\Users\\olanr\\miniconda3\\envs\\data-analysis\\lib\\site-packages\\branca\\colormap.py:122\u001b[0m, in \u001b[0;36mColorMap.rgba_bytes_tuple\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrgba_bytes_tuple\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    119\u001b[0m     \u001b[39m\"\"\"Provides the color corresponding to value `x` in the\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m    form of a tuple (R,G,B,A) with int values between 0 and 255.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\u001b[39mint\u001b[39m(u\u001b[39m*\u001b[39m\u001b[39m255.9999\u001b[39m) \u001b[39mfor\u001b[39;00m u \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrgba_floats_tuple(x))\n",
      "File \u001b[1;32mc:\\Users\\olanr\\miniconda3\\envs\\data-analysis\\lib\\site-packages\\branca\\colormap.py:256\u001b[0m, in \u001b[0;36mLinearColormap.rgba_floats_tuple\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    254\u001b[0m     p \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n\u001b[0;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThresholds are not sorted.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m((\u001b[39m1.\u001b[39m\u001b[39m-\u001b[39mp) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolors[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][j] \u001b[39m+\u001b[39m p\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolors[i][j] \u001b[39mfor\u001b[39;00m j\n\u001b[0;32m    259\u001b[0m              \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Thresholds are not sorted."
     ]
    }
   ],
   "source": [
    "D.plot_trajectories()\n",
    "D.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faccf2a0d0d6f24ad133c954b727a3417ecb6a1c20419487847b75b7fcaef0dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
