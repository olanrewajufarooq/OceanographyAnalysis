{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "# Python-based Libraries\n",
    "from datetime import timedelta, datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Data Analysis Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "# Graph plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplcols\n",
    "from matplotlib import cm\n",
    "import folium\n",
    "import branca.colormap as bcm\n",
    "\n",
    "    \n",
    "# Filtering Data before Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drifters:\n",
    "    \n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "        self.data = {}\n",
    "        self.data_info = {}\n",
    "        \n",
    "    def read_data(self, path):\n",
    "        # Define Possible encodings\n",
    "        encodings = [\"UTF-8\", \"UTF-16 LE\"]\n",
    "\n",
    "        # Check if path is a filepath or directory path\n",
    "        if Path(path).is_file(): \n",
    "            # Get file name from path\n",
    "            filename = path.split('/')[-1].split('.')[-2] \n",
    "            # Define a variable to track file decoding\n",
    "            encoding_gotten = False\n",
    "            for encoding_value in encodings:\n",
    "                try:\n",
    "                    data_value = pd.read_csv(path, encoding=encoding_value)\n",
    "                    # Catch special errors whereby the encoding is wrong but the data is read.\n",
    "                    if \"Unnamed: 1\" in data_value.columns: \n",
    "                        continue\n",
    "                    encoding_gotten = True\n",
    "                    break # Stop trying if the options had worked\n",
    "                except:\n",
    "                    pass # Continue trying if the option didn't work\n",
    "\n",
    "            if encoding_gotten == True:\n",
    "                data_to_keep = [['DeviceDateTime','Latitude','Longitude'], \n",
    "                            ['Position time (UTC)', 'Latitude (째)', 'Longitude (째)'],\n",
    "                            ]\n",
    "                for dtk_val in data_to_keep:\n",
    "                    try:\n",
    "                        # Select Data to Keep\n",
    "                        data_value = data_value[dtk_val]\n",
    "                        # Drop columns with missing values\n",
    "                        data_value.dropna()\n",
    "                        # Rename Header Columns\n",
    "                        data_value.columns = ['DateTime','Latitude','Longitude']\n",
    "                        # Convert DateTime\n",
    "                        data_value['DateTime'] = pd.to_datetime(data_value['DateTime'])\n",
    "                        # Store DF in the object instance 'data'\n",
    "                        self.data[filename] = data_value\n",
    "                        \n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                print(f\"Encoding error. The encoding for {filename} data should be checked.\")\n",
    "\n",
    "        elif Path(path).is_dir():    \n",
    "        # 1. Read the files in the given path\n",
    "            file_names = os.listdir(path)\n",
    "\n",
    "            # 2. Read each file into pandas dataframe\n",
    "            for file in file_names:\n",
    "                filepath = path+'/'+file\n",
    "                filename = file.split('.')[-2] \n",
    "                # Define a variable to track file decoding\n",
    "                encoding_gotten = False\n",
    "                for encoding_value in encodings:\n",
    "                    try:\n",
    "                        data_value = pd.read_csv(filepath, encoding=encoding_value)\n",
    "                        # Catch special errors whereby the encoding is wrong but the data is read.\n",
    "                        if \"Unnamed: 1\" in data_value.columns: \n",
    "                            continue\n",
    "                        encoding_gotten = True\n",
    "                        break # Stop trying if the options had worked\n",
    "                    except:\n",
    "                        pass # Continue trying if the option didn't work\n",
    "\n",
    "                # Store the data in the \"data\" dictionary\n",
    "                if encoding_gotten == True:\n",
    "                    data_to_keep = [['DeviceDateTime','Latitude','Longitude'], \n",
    "                               ['Position time (UTC)', 'Latitude (째)', 'Longitude (째)'],\n",
    "                                ]\n",
    "                    for dtk_val in data_to_keep:\n",
    "                        try:\n",
    "                            # Select Data to Keep\n",
    "                            data_value = data_value[dtk_val]\n",
    "                            # Drop columns with missing values\n",
    "                            data_value.dropna(axis=0, inplace=True)\n",
    "                            # Rename Header Columns\n",
    "                            data_value.columns = ['DateTime','Latitude','Longitude']\n",
    "                            # Convert DateTime\n",
    "                            data_value['DateTime'] = pd.to_datetime(data_value['DateTime'])\n",
    "                            # Store DF in the object instance 'data'\n",
    "                            self.data[filename] = data_value\n",
    "                        \n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                else:\n",
    "                    print(f\"Encoding error. The encoding for {filepath} data should be checked.\")\n",
    "    \n",
    "    def read_logsheet(self, file_path):\n",
    "        \n",
    "        self.logsheet = pd.read_csv(file_path)\n",
    "        \n",
    "        self.logsheet.dropna(axis=0, inplace=True)\n",
    "        \n",
    "        self.logsheet[\"DepDateTime\"] = self.logsheet.apply(lambda x: str(x[\"DepDate\"]) + \" \" + str(x[\"DepTime\"]), axis = 1)\n",
    "        self.logsheet[\"RecovDateTime\"] = self.logsheet.apply(lambda x: str(x[\"RecovDate\"]) + \" \" + str(x[\"RecovTime\"]), axis = 1)\n",
    "        \n",
    "        self.logsheet.drop(columns=[\"DepDate\", \"DepTime\", \"RecovDate\", \"RecovTime\"], inplace=True)\n",
    "        \n",
    "        self.logsheet[\"DepDateTime\"] = pd.to_datetime(self.logsheet[\"DepDateTime\"])\n",
    "        self.logsheet[\"RecovDateTime\"] = pd.to_datetime(self.logsheet[\"RecovDateTime\"])\n",
    "        \n",
    "        for key in self.data.keys():\n",
    "            station = self.logsheet[\"Station\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            drog_depth = self.logsheet[\"DrogDepth\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            \n",
    "            dep_datetime = self.logsheet[\"DepDateTime\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            recov_datetime = self.logsheet[\"RecovDateTime\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            dep_long = self.logsheet[\"DepLong\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            dep_lat = self.logsheet[\"DepLat\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            recov_long = self.logsheet[\"RecovLong\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            recov_lat = self.logsheet[\"RecovLat\"][self.logsheet[\"Name\"] == float(key)].values\n",
    "            \n",
    "            \n",
    "            info = {\"Station\":station, \"DrogDepth\":drog_depth, \"DepDateTime\":dep_datetime, \"RecovDateTime\":recov_datetime,\n",
    "                    \"DepLong\":dep_long, \"DepLat\":dep_lat, \"RecovLong\":recov_long, \"RecovLat\":recov_lat}\n",
    "            \n",
    "            self.data_info[key] = info\n",
    "    \n",
    "    def time_shift(self, shift_amount:float = 1):\n",
    "        for key in self.data.keys():\n",
    "            self.data[key][\"DateTime\"] = self.data[key][\"DateTime\"] + timedelta(hours = shift_amount)\n",
    "    \n",
    "    def extract_data(self):\n",
    "        for key in self.data_info.keys():\n",
    "            # Get the start and end time for each experiment\n",
    "            deploy_time = self.data_info[key]['DepDateTime']\n",
    "            recov_time = self.data_info[key]['RecovDateTime']\n",
    "\n",
    "            # Extract experiment data within the start and end time \n",
    "            # bool = np.array([(deploy_time < self.data[key][\"DateTime\"][i] < recov_time) for i in range(len(self.data[key]))]).flatten()\n",
    "            bool = np.array([(deploy_time < pos_time < recov_time) for pos_time in self.data[key][\"DateTime\"]]).flatten()\n",
    "            \n",
    "            if bool.size != 0:\n",
    "                self.data[key] = self.data[key][bool]\n",
    "                # Drop columns with missing values\n",
    "                self.data[key].dropna(axis=0, inplace=True)\n",
    "                # Reset Index\n",
    "                self.data[key].reset_index(inplace = True)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def compute_velocity(self):\n",
    "        R = 6373.0\n",
    "        for key in self.data.keys():\n",
    "            # Sort the DF by Ascending Datetime\n",
    "            self.data[key] = self.data[key].sort_values(by=[\"DateTime\"])\n",
    "            # self.data[key].sort_values(by=[\"DateTime\"], inplace=True)\n",
    "            \n",
    "            # select columns for latitude, longtitude and time\n",
    "            df_lat = np.array(self.data[key][\"Latitude\"])\n",
    "            df_long = np.array(self.data[key][\"Longitude\"])\n",
    "            df_time = self.data[key][\"DateTime\"]\n",
    "            \n",
    "            # compute delta time\n",
    "            d_time = np.array(df_time[1:]) - df_time[:-1]\n",
    "            d_time = d_time.apply(lambda dt_i : dt_i.seconds)\n",
    "            # Compute time\n",
    "            distance = np.array([haversine((df_lat[i], df_long[i]), (df_lat[i+1], df_long[i+1]), unit=Unit.METERS) for i in range(df_lat.size - 1)])\n",
    "            # Compute velocity\n",
    "            velocity = distance*100/(np.array(d_time, dtype=float))\n",
    "            # Remove the first row\n",
    "            self.data[key]= self.data[key].iloc[1:]\n",
    "            self.data[key][\"Velocity\"] = pd.Series(velocity)\n",
    "            \n",
    "            # Drop columns with missing values\n",
    "            self.data[key].dropna(axis=0, inplace=True)\n",
    "            # Reset Index\n",
    "            self.data[key].reset_index(inplace = True)\n",
    "\n",
    "    def plot_trajectories(self, saving_path=None):\n",
    "        \n",
    "        def velocity_extremes(data):\n",
    "            min_vel = np.inf\n",
    "            max_vel = -np.inf\n",
    "\n",
    "            for key in data.keys():\n",
    "                min_vel_val = np.min(data[key][\"Velocity\"])\n",
    "                max_vel_val = np.max(data[key][\"Velocity\"])\n",
    "\n",
    "                min_vel = min_vel_val if min_vel_val < min_vel else min_vel\n",
    "                max_vel = max_vel_val if max_vel_val > max_vel else max_vel\n",
    "\n",
    "            return min_vel, max_vel\n",
    "        \n",
    "        def graph_centre(data):\n",
    "            min_lat = 100\n",
    "            max_lat = 0\n",
    "            min_lon = 100\n",
    "            max_lon = 0\n",
    "\n",
    "            for key in data.keys():\n",
    "                min_lat_val = np.min(data[key][\"Latitude\"])\n",
    "                min_lon_val = np.min(data[key][\"Longitude\"])\n",
    "                max_lat_val = np.max(data[key][\"Latitude\"])\n",
    "                max_lon_val = np.max(data[key][\"Longitude\"])\n",
    "\n",
    "                min_lat = min_lat_val if min_lat_val < min_lat else min_lat\n",
    "                max_lat = max_lat_val if max_lat_val > max_lat else max_lat\n",
    "                min_lon = min_lon_val if min_lon_val < min_lon else min_lon\n",
    "                max_lon = max_lon_val if max_lon_val > max_lon else max_lon\n",
    "\n",
    "            lat_centre = np.average([min_lat, max_lat])\n",
    "            lon_centre= np.average([min_lon, max_lon])\n",
    "\n",
    "            return lat_centre, lon_centre\n",
    "        \n",
    "        def plot_vel_traj(df_map, df, name, drog_depth, color_mapper):\n",
    "            \n",
    "            colors = ['#ffff00', '#ffffff', '#0000ff']\n",
    "            drog_depths = np.unique(self.logsheet[\"DrogDepth\"])\n",
    "            \n",
    "            drog_depth_found = False\n",
    "            count = 0\n",
    "            for depth in drog_depths:\n",
    "                if drog_depth == depth:\n",
    "                    drifter_col, drifter_info = [colors[count], f'{drog_depth}m Depth']\n",
    "                    drog_depth_found = False\n",
    "                    break\n",
    "                count += 1\n",
    "                \n",
    "            if not drog_depth_found:\n",
    "                drifter_col, drifter_info = ['#000000', 'Unknown']\n",
    "                \n",
    "            feature_group = folium.FeatureGroup(f\"{name}\\n{drifter_info}\")\n",
    "            \n",
    "            folium.Marker([ df[\"Latitude\"].iloc[-1]+0.001, df[\"Longitude\"].iloc[-1]-0.001 ],\n",
    "                    icon=folium.DivIcon(html=f\"\"\"<div style=\"font-family: courier new; color: blue\">{f\"{name}\"}</div>\"\"\")\n",
    "                    ).add_to(feature_group )\n",
    "            \n",
    "            start = False\n",
    "\n",
    "            for lat, lon in zip(df[\"Latitude\"], df[\"Longitude\"]):\n",
    "                if start: line = folium.PolyLine([[prev_lat, prev_long], [lat, lon]], color='white', weight=2).add_to(feature_group )\n",
    "                else: start = True\n",
    "\n",
    "                prev_lat = lat\n",
    "                prev_long = lon\n",
    "\n",
    "\n",
    "            for lat, lon, vel in zip(df[\"Latitude\"], df[\"Longitude\"], df[\"Velocity\"]):\n",
    "                circlemarker = folium.CircleMarker(location=(lat,lon), radius=6,\n",
    "                                    stroke=True, color=drifter_col, weight=1.5, opacity=0.9,  \n",
    "                                    fill=True, fill_color=color_mapper(vel)[:7], fill_opacity=0.9,\n",
    "                                    popup=folium.Popup(f\"{round(vel, 4)} m/s\"),\n",
    "                                ).add_to(feature_group )\n",
    "            \n",
    "            feature_group.add_to(df_map)\n",
    "\n",
    "            return df_map\n",
    "        \n",
    "        def visualize_drifters(data, data_info):\n",
    "            \n",
    "            # Initializing the Map\n",
    "            plot_map = folium.Map(location = graph_centre(data), zoom_start = 14.5, position=\"absolute\", width='100%', height=\"100%\", \n",
    "                                left='0%', top='0%', border=None, min_zoom=13, max_zoom=17)\n",
    "\n",
    "            # position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen\n",
    "\n",
    "            # Plotting the location and velocities into the Map\n",
    "            min_vel, max_vel = velocity_extremes(data)\n",
    "\n",
    "            color_gradients = ['white', 'green', 'orange', 'red']\n",
    "            velcolmap = bcm.LinearColormap(color_gradients, vmin=min_vel, vmax=max_vel, caption = 'Velocity (cm/s)') # The velocity scale\n",
    "            velcolmap.add_to(plot_map)\n",
    "\n",
    "            for key in data.keys():\n",
    "                drog_depth = data_info[key][\"DrogDepth\"]\n",
    "                plot_map = plot_vel_traj(plot_map, data[key], key, drog_depth, velcolmap)\n",
    "\n",
    "            folium.LayerControl().add_to(plot_map)\n",
    "            return plot_map\n",
    "        \n",
    "        self.map = visualize_drifters(self.data, self.data_info)\n",
    "        \n",
    "        if saving_path:\n",
    "            self.map.save(f\"{saving_path}/Graphs/TrajAndVelocity.html\")\n",
    "            \n",
    "    def plot_boxplot(self):\n",
    "        pass\n",
    "    \n",
    "    def plot_lineplot(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code\n",
    "working_dir = \"Data/2022/Day2\"\n",
    "D = Drifters(\"2022\")\n",
    "D.read_data(f\"{working_dir}/drifters/\")\n",
    "D.read_logsheet(f\"{working_dir}/drifters-logsheet.csv\")\n",
    "\n",
    "key = \"274\"\n",
    "\n",
    "D.data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.logsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.time_shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.data[key].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.extract_data()\n",
    "D.data[key].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.time_shift(shift_amount=2)\n",
    "D.data[key].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.compute_velocity()\n",
    "D.data[key].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.plot_trajectories()\n",
    "D.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022 Drifter Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Day 1 Parameters\n",
    "working_dir = \"Data/2022/Day1\"\n",
    "\n",
    "# Create an Instance of the Drifter Class\n",
    "D1 = Drifters(\"2022-1\")\n",
    "\n",
    "# Read the Data\n",
    "D1.read_data(f\"{working_dir}/drifters/\")\n",
    "\n",
    "# Read the Logsheet\n",
    "D1.read_logsheet(f\"{working_dir}/drifters-logsheet.csv\")\n",
    "\n",
    "# Shift time for the data\n",
    "D1.time_shift(2)\n",
    "\n",
    "# Extract Valid Data\n",
    "D1.extract_data()\n",
    "\n",
    "# Compute Velocity\n",
    "D1.compute_velocity()\n",
    "\n",
    "# Visualize the first 5 items of the modified data\n",
    "D1.data['0119']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Day 1 Parameters\n",
    "working_dir = \"Data/2022/Day2\"\n",
    "\n",
    "# Create an Instance of the Drifter Class\n",
    "D2 = Drifters(\"2022-2\")\n",
    "\n",
    "# Read the Data\n",
    "D2.read_data(f\"{working_dir}/drifters/\")\n",
    "\n",
    "# Read the Logsheet\n",
    "D2.read_logsheet(f\"{working_dir}/drifters-logsheet.csv\")\n",
    "\n",
    "# Shift time for the data\n",
    "D2.time_shift(2)\n",
    "\n",
    "# Extract Valid Data\n",
    "D2.extract_data()\n",
    "\n",
    "# Compute Velocity\n",
    "D2.compute_velocity()\n",
    "\n",
    "# Visualize the first 5 items of the modified data\n",
    "D2.data['0119']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Day 1 Parameters\n",
    "working_dir = \"Data/2022/Day3\"\n",
    "\n",
    "# Create an Instance of the Drifter Class\n",
    "D3 = Drifters(\"2022-2\")\n",
    "\n",
    "# Read the Data\n",
    "D3.read_data(f\"{working_dir}/drifters/\")\n",
    "\n",
    "# Read the Logsheet\n",
    "D3.read_logsheet(f\"{working_dir}/drifters-logsheet.csv\")\n",
    "\n",
    "# Shift time for the data\n",
    "D3.time_shift(2)\n",
    "\n",
    "# Extract Valid Data\n",
    "D3.extract_data()\n",
    "\n",
    "# Compute Velocity\n",
    "D3.compute_velocity()\n",
    "\n",
    "# Visualize the first 5 items of the modified data\n",
    "D3.data['0119']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oceanography-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c7e9be91334992b2c4a8113cde6d2dd90495863aed28cc092af08fc99f3a76b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
